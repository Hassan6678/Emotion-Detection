{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "print(\"Model archticture: \", net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincsv_file = './dataset'+'/'+'train.csv'\n",
    "validationcsv_file = './dataset'+'/'+'val.csv'\n",
    "train_img_dir = './dataset'+'/'+'train/'\n",
    "validation_img_dir = './dataset'+'/'+'val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        print(\"===================================Start Validate===================================\")\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_t = pd.read_csv(validationcsv_file,usecols=['emotion','pixels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.to_pickle('val_data.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data(data_file):\n",
    "    print('Loading data ...')\n",
    "    with open(data_file, 'rb') as f:\n",
    "        pickle_data = pickle.load(f)\n",
    "        x_data = pickle_data['pixels']\n",
    "        y_data = pickle_data['emotion']\n",
    "    print('Data loaded.')\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Data loaded.\n",
      "Total samples: 25120\n",
      "images shape: (25120,)\n",
      "labels shape: (25120,)\n"
     ]
    }
   ],
   "source": [
    "data_file = './train.p'\n",
    "images, labels = load_data(data_file)\n",
    "\n",
    "n_samples = labels.shape[0]\n",
    "print('Total samples:', n_samples)\n",
    "print('images shape:', images.shape)\n",
    "print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:4066: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n",
      "c:\\Users\\Hassan Raza\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Start Validate===================================\n",
      "Epoch: 1 \tTraining Loss: 0.22638617 \tValidation Loss 0.22697815 \tTraining Acuuarcy 24.972% \tValidation Acuuarcy 25.160%\n",
      "===================================Start Validate===================================\n",
      "Epoch: 2 \tTraining Loss: 0.22660394 \tValidation Loss 0.22590310 \tTraining Acuuarcy 25.127% \tValidation Acuuarcy 25.160%\n",
      "===================================Start Validate===================================\n",
      "Epoch: 3 \tTraining Loss: 0.22657497 \tValidation Loss 0.22603243 \tTraining Acuuarcy 25.127% \tValidation Acuuarcy 25.160%\n",
      "===================================Start Validate===================================\n",
      "Epoch: 4 \tTraining Loss: 0.22659185 \tValidation Loss 0.22581725 \tTraining Acuuarcy 25.127% \tValidation Acuuarcy 25.160%\n",
      "===================================Start Validate===================================\n",
      "Epoch: 5 \tTraining Loss: 0.22652069 \tValidation Loss 0.22595964 \tTraining Acuuarcy 25.127% \tValidation Acuuarcy 25.160%\n",
      "===================================Training Finished===================================\n"
     ]
    }
   ],
   "source": [
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "753cedd98399b984ff7ce64c091fff2e66dee429919726e503dc47428fd50bcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
